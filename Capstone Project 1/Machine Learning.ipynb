{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Actions 1: Read the dataset from csv file. Create dummy variables for the nine levels of rural_urbal_continuum_codes (RUCC) and the four geographical regions. Create the predictors DataFrame and the target variable Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>less_than_high_school</th>\n",
       "      <th>high_school_diploma</th>\n",
       "      <th>college/associate_degree</th>\n",
       "      <th>unemployment</th>\n",
       "      <th>region_Northeast</th>\n",
       "      <th>region_South</th>\n",
       "      <th>region_West</th>\n",
       "      <th>RUCC_1</th>\n",
       "      <th>RUCC_2</th>\n",
       "      <th>RUCC_3</th>\n",
       "      <th>RUCC_4</th>\n",
       "      <th>RUCC_5</th>\n",
       "      <th>RUCC_6</th>\n",
       "      <th>RUCC_7</th>\n",
       "      <th>RUCC_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.417</td>\n",
       "      <td>34.331</td>\n",
       "      <td>28.660</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.972</td>\n",
       "      <td>28.692</td>\n",
       "      <td>31.788</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.236</td>\n",
       "      <td>34.927</td>\n",
       "      <td>25.969</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.302</td>\n",
       "      <td>41.816</td>\n",
       "      <td>26.883</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.969</td>\n",
       "      <td>32.942</td>\n",
       "      <td>34.039</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   less_than_high_school  high_school_diploma  college/associate_degree  \\\n",
       "0                 12.417               34.331                    28.660   \n",
       "1                  9.972               28.692                    31.788   \n",
       "2                 26.236               34.927                    25.969   \n",
       "3                 19.302               41.816                    26.883   \n",
       "4                 19.969               32.942                    34.039   \n",
       "\n",
       "   unemployment  region_Northeast  region_South  region_West  RUCC_1  RUCC_2  \\\n",
       "0           5.3                 0             1            0       0       1   \n",
       "1           5.4                 0             1            0       0       0   \n",
       "2           8.6                 0             1            0       0       0   \n",
       "3           6.6                 0             1            0       1       0   \n",
       "4           5.5                 0             1            0       1       0   \n",
       "\n",
       "   RUCC_3  RUCC_4  RUCC_5  RUCC_6  RUCC_7  RUCC_8  \n",
       "0       0       0       0       0       0       0  \n",
       "1       1       0       0       0       0       0  \n",
       "2       0       0       0       1       0       0  \n",
       "3       0       0       0       0       0       0  \n",
       "4       0       0       0       0       0       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are many warnings regarding the updates in the future releases of the libraries. Ignore them.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Improt necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv('wrangled_data.csv')\n",
    "# Remove the .0 after RUCC numbers and transform it to string\n",
    "df.RUCC=df.RUCC.astype('int').astype('str')\n",
    "\n",
    "# define the predictors DataFrame and the target variable Series\n",
    "X = df.drop(['state','county','poverty'],axis=1)\n",
    "y = df['poverty']\n",
    "\n",
    "# Create dummy variables for region and RUCC\n",
    "X=pd.get_dummies(X)\n",
    "\n",
    "# drop one column of dummy variable from each categorical variable to avoid collinearity\n",
    "# In Story Telling section, it was shown that the correlations between poverty and majority of rural urban continuum codes \n",
    "# i.e. all except RUCC 1 and 6, are close to each other . Therefore, any of these seven RUCC (2-5,7-9) could be picked as the \n",
    "# base to simplify the interpretation. I pick RUCC 9 and remove it\n",
    "# In Story Telling section, it was shown that the correlations between poverty and the four geographical regions are different\n",
    "# from each other. Therefore, any of them could be picked as the base. I pick Midwest and remove it\n",
    "# Also, the sum of the four education levels is 100 and one of them must be removed to avoid collinearity.\n",
    "# In Story Telling section, it was shown that the correlations between poverty and the four education levels are different\n",
    "# from each other. Therefore, any of them could be picked as the base. I pick bachelors/higher and remove it\n",
    "\n",
    "X.drop(['region_Midwest','RUCC_9','bachelors/higher'],axis=1,inplace=True)\n",
    "\n",
    "#Print the first five rows of X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build a linear model: there are 15 predictors and more than 3000 samples; therefore,the model can be built based on all predictors. Then, run 5-fold cross-validation and average the scores. At the end, fit the linear model on the entire dataset and measure its R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of 5-fold cross-validation scores for the linear model is 0.525\n",
      "The R-squared of the linear model fitted on the entire dataset is 0.596\n"
     ]
    }
   ],
   "source": [
    "# Build the linear model \n",
    "model = LinearRegression()\n",
    "\n",
    "# Run cross-validation and print the average of scores\n",
    "scores = cross_val_score(model,X,y,cv=5)\n",
    "print('The average of 5-fold cross-validation scores for the linear model is %.3f'%np.mean(scores))\n",
    "\n",
    "# Fit the model on the entire dataset and print its R-squared\n",
    "model.fit(X,y)\n",
    "print ('The R-squared of the linear model fitted on the entire dataset is %.3f'%model.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "#### The linear model does not have a high R-squared. It also does not have a good performance in prediction since the average of cross-validation scores is not high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add interaction between predictors and nonlinear terms to the linear model. Then, run 5-fold cross-validation and average the scores. At the end, fit the linear model on the entire dataset and measure its R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of 5-fold cross-validation scores for the linear model which includes nonlinear terms as well as interaction between predictors is 0.564\n",
      "The R-squared of the linear model with interaction and nonlinear terms fitted on the entire dataset is 0.671\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression and polynomial features, and use pipeline\n",
    "linear = LinearRegression()\n",
    "poly = PolynomialFeatures(degree = 2, interaction_only = False)\n",
    "pipeline = make_pipeline(poly, linear)\n",
    "\n",
    "# Run cross-validation and print the results\n",
    "scores = cross_val_score(pipeline,X,y,cv=5)\n",
    "print('The average of 5-fold cross-validation scores for the linear model which includes nonlinear terms\\\n",
    " as well as interaction between predictors is %.3f'%np.mean(scores))\n",
    "\n",
    "# Fit the model on the entire data and print its R-squared\n",
    "pipeline.fit(X,y)\n",
    "print ('The R-squared of the linear model with interaction and nonlinear terms fitted on the entire dataset is %.3f'\\\n",
    "       %pipeline.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "#### Both R-squared of the linear model and its performance in prediction (average of cross-validation scores) have slightly improved after adding nonlinear and interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 3: Random Forest: first, build a random forest with no parameter tuning, run 5-fold cross-validation, and average the scores to understand the impact of parameter tuning. Then, divide the dataset into train and test sets and tune some parameters of the model based on the train set. At the end, the performance of model in predicting the target value of the test set will be computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of 5-fold cross-validation scores for the random forest without parameter tuning is 0.512\n"
     ]
    }
   ],
   "source": [
    "# Build the model without parameter tuning, run cross-validation, and average the scores\n",
    "forest = RandomForestRegressor(random_state=21)\n",
    "scores = cross_val_score(forest,X,y,cv=5)\n",
    "print('The average of 5-fold cross-validation scores for the random forest without parameter tuning is %.3f'%np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which receives model and parameters as inputs, tune parameters \n",
    "# and prints best score, best parameters and the test-set score for the model created with the best parameters\n",
    "def tuning(forest,parameters):\n",
    "    model = GridSearchCV(forest,param_grid=parameters,cv=5)\n",
    "    model.fit(X_train,y_train)\n",
    "    print('The best score in parameter tuning is: %.3f'%model.best_score_)\n",
    "    print('The best parameters are: ',model.best_params_)    \n",
    "    print('The test-set score after tuning parameters is %.3f'%model.score(X_test,y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score in parameter tuning is: 0.628\n",
      "The best parameters are:  {'criterion': 'mse', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "The test-set score after tuning parameters is 0.632\n"
     ]
    }
   ],
   "source": [
    "# Build the random forest, Tune its parameters and find the test-set score for the new model\n",
    "forest = RandomForestRegressor(random_state=21)\n",
    "parameters = {'n_estimators':[10,50,100],\\\n",
    "              'criterion': ['mse','mae'],\\\n",
    "             'min_samples_leaf':[2,5,10],\\\n",
    "             'max_features':['auto','sqrt','log2']}\n",
    "model = tuning(forest,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The performance of the random forest in prediction has significantly improved after tuning parameters because its test-set score  (0.632) is higher than the average of 5-fold cross-validation scores for the random forest without parameter tuning (0.512) \n",
    "\n",
    "#### The best values of n_estimators and min_samples_leaf are either lowest or highest one among the tested values. Therefore, some new values for these two parameters are examined to find the best values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score in parameter tuning is: 0.634\n",
      "The best parameters are:  {'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "The test-set score after tuning parameters is 0.619\n"
     ]
    }
   ],
   "source": [
    "# Build new random forest with 'mse' as criterion and 'sqrt' as max_features, tune n_estimators and min_samples_leaf, \n",
    "# and find the test-set score for the new model\n",
    "forest = RandomForestRegressor(criterion = 'mse', max_features = 'sqrt', random_state=21)\n",
    "parameters = {'n_estimators':[70,100,300,500],'min_samples_leaf':[1,2,3]}\n",
    "#parameters = {'n_estimators':[70,100,300],'min_samples_split':[2,5]}\n",
    "model = tuning(forest,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_samples_leaf equal to 1 results in higher best score in parameter tuning but lower test-set score compared to min_samples_leaf equal to 2. In other words, the value 1 causes overfitting. Therefore, I keep min_samples_leaf equal to 2 and test some values for n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score in parameter tuning is: 0.632\n",
      "The best parameters are:  {'n_estimators': 500}\n",
      "The test-set score after tuning parameters is 0.634\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestRegressor(criterion = 'mse', max_features = 'sqrt', min_samples_leaf = 2, random_state=21)\n",
    "parameters = {'n_estimators':[70,100,300,500]}\n",
    "model = tuning(forest,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators equal to 500 results in only 0.002 improvment in the test-set score compared to n_estimators equal to 100 (0.634 compared to 0.632). As a result, it is not necessary to test values beyond 500 for n_estimators since it will make the model more complicated but will not make significant increase in the test-set score. Therefore, the tuned parameters are  criterion = 'mse', max_features = 'sqrt', min_samples_leaf = 2, and n_estimators = 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At the end, I will find the R-squared of the model fitted on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared of the random forest with tuned parameters fitted on the entire dataset is 0.691\n"
     ]
    }
   ],
   "source": [
    "#Build the random forest with tuned parameters\n",
    "model = RandomForestRegressor(n_estimators = 500, min_samples_leaf = 10,criterion = 'mse', max_features = 'sqrt', random_state=21)\n",
    "\n",
    "#Fit the model to the entire dataset and find the R-squared \n",
    "model.fit(X,y)\n",
    "print('The R-squared of the random forest with tuned parameters fitted on the entire dataset is %.3f'%model.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "#### 1) The performance of the the random forest model in prediction after tuning its parameters has improved compared to the linear model with interaction and nonlinear terms because the test-set score of the random forest model is 0.634 but the average of 5-fold cross-validation scores of the linear model is 0.564.\n",
    "#### 2) The R-sqaured of the the random forest model (with tuned parameters) fitted on the entire dataset is 0.857 which is more than R-squared of the linear model (with interaction and non-linear terms) which is 0.671."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Improve random forest performance by adding one more predictor to the predictors DataFrame.  In order to create this predictor, the dataset is clustered. The cluster is the new predictor. The number of clusters is a parameter which will be tuned along with the random forest parameters by grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   First, I define a function which first generates clusters for the train set. Then, it will predcit clusters for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define clustering function\n",
    "def clustering(X_train,X_test,n_cluster):\n",
    "    \n",
    "    # Build the KMeans cluster, fit it to the train set, create the new predictor for the train set, \n",
    "    # and cocatenate it to the predictors DataFrame \n",
    "    kmeans = KMeans(n_clusters=n_cluster,random_state=21)\n",
    "    kmeans.fit(X_train)\n",
    "    new_column_train = kmeans.predict(X_train) \n",
    "    new_df_train = pd.DataFrame(new_column_train,columns = ['cluster'], index = X_train.index)\n",
    "    X_train_extended = pd.concat([X_train,new_df_train],axis=1)\n",
    "    \n",
    "    # Build the new predictor for the test set and concatenate it to the predictors DataFrame of the test set\n",
    "    new_column_test = kmeans.predict(X_test) \n",
    "    new_df_test = pd.DataFrame(new_column_test,columns = ['cluster'], index = X_test.index)\n",
    "    X_test_extended = pd.concat([X_test,new_df_test],axis=1)\n",
    "    \n",
    "    # Return the new predictors DataFrame for both train and test sets\n",
    "    return X_train_extended,X_test_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then I define a function which runs the grid search and finds the best values for number of clusters, n_estimators, and min_samples_leaf (criterion and max_features will be set to 'mse' and 'sqrt', respectively). Then, it will print the best score of parameter tuning, best parameters and the test-set score for the model created with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_forest_SearchCV(cluster_numbers,n_estimators,min_samples_leaf,X,y):\n",
    "    # Split dataset into train and test set.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=21)\n",
    "    # Create KFold with 5 as the number of splits.\n",
    "    kf = KFold(n_splits = 5)\n",
    "    # Create a zero array which will hold the score of each split run. Since the number of splits is 5, the first dimention is 5\n",
    "    a = np.zeros(shape = (5,len(cluster_numbers),len(n_estimators),len(min_samples_leaf)))\n",
    "    \n",
    "    l = 0\n",
    "    # Loop over the splits of the train set  \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_train_kf = X_train.iloc[train_index]\n",
    "        X_test_kf = X_train.iloc[test_index]\n",
    "        y_train_kf = y_train.iloc[train_index]\n",
    "        y_test_kf = y_train.iloc[test_index]\n",
    "        \n",
    "        # Loop over number of clusters\n",
    "        for i,n_cluster in enumerate(cluster_numbers):\n",
    "            # Cluster each dataset in train set \n",
    "            X_train_kf_extended,X_test_kf_extended = clustering(X_train_kf,X_test_kf,n_cluster)\n",
    "            # Loop over n_estimators\n",
    "            for j,n_estimator in enumerate(n_estimators):\n",
    "                # Loop over min_samples_leaf\n",
    "                for k,min_sample_leaf in enumerate(min_samples_leaf):\n",
    "                    # Create the random forest and find the score\n",
    "                    model = RandomForestRegressor(n_estimators = n_estimator, min_samples_leaf = min_sample_leaf, \\\n",
    "                                                   criterion = 'mse', max_features = 'sqrt', random_state=21)\n",
    "                    model.fit(X_train_kf_extended,y_train_kf)\n",
    "                    score = model.score(X_test_kf_extended,y_test_kf)\n",
    "                    # Save the score in the array\n",
    "                    a[l,i,j,k] = score\n",
    "        l += 1\n",
    "     \n",
    "    # Average the scores \n",
    "    average_score = np.sum(a,axis = 0)/5\n",
    "    \n",
    "    #print(average_score)\n",
    "    \n",
    "    #Find the highest average score\n",
    "    max_score = np.amax(average_score)\n",
    "    \n",
    "    # Find the cluster_number, n_estimator, and min_sample_leaf corresponding to the best score\n",
    "    max_score_index = np.unravel_index(average_score.argmax(), average_score.shape)\n",
    "    cluster_number = cluster_numbers[max_score_index[0]]\n",
    "    n_estimator = n_estimators[max_score_index[1]]\n",
    "    min_sample_leaf = min_samples_leaf[max_score_index[2]]\n",
    "    \n",
    "    # Print the best score of parameter tuning and best parameters\n",
    "    print('The best score is %.3f'%max_score)\n",
    "    print('Number of clusters for the best score is ',cluster_number)\n",
    "    print('n_estimators for the best score is ',n_estimator)\n",
    "    print('min_samples_leaf for the best score is ',min_sample_leaf)\n",
    "    \n",
    "    # Cluster the train and test sets of the dataset\n",
    "    X_train_extended,X_test_extended = clustering(X_train,X_test,cluster_number)\n",
    "    \n",
    "    # Build the random forest with the tuned parameters and fit it to the train set\n",
    "    model = RandomForestRegressor(n_estimators = n_estimator, min_samples_leaf = min_sample_leaf, \\\n",
    "                                                   criterion = 'mse', max_features = 'sqrt', random_state=21)\n",
    "    model.fit(X_train_extended,y_train)\n",
    "    \n",
    "    # Compute the test-set score for the random forest and print the score\n",
    "    score = model.score(X_test_extended,y_test)\n",
    "    print('The test set score after tuning the parameters is %.3f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the parameters and find the test-set score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is 0.636\n",
      "Number of clusters for the best score is  5\n",
      "n_estimators for the best score is  300\n",
      "min_samples_leaf for the best score is  2\n",
      "The test set score after tuning the parameters is 0.635\n"
     ]
    }
   ],
   "source": [
    "# Tune the parameters of the random forest fitted on the data with cluster as the new predictor\n",
    "cluster_numbers = [1,2,5]\n",
    "n_estimators = [100,300]\n",
    "min_samples_leaf = [1,2,5]\n",
    "cluster_forest_SearchCV(cluster_numbers,n_estimators,min_samples_leaf,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is 0.636\n",
      "Number of clusters for the best score is  50\n",
      "n_estimators for the best score is  500\n",
      "min_samples_leaf for the best score is  2\n",
      "The test set score after tuning the parameters is 0.632\n"
     ]
    }
   ],
   "source": [
    "# Tune the parameters of the random forest fitted on the data with cluster as the new predictor\n",
    "cluster_numbers = [5,10,50]\n",
    "n_estimators = [300,500]\n",
    "min_samples_leaf = [1,2,5]\n",
    "cluster_forest_SearchCV(cluster_numbers,n_estimators,min_samples_leaf,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At the end, I will find the R-squared of the model fitted on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared of the random forest with tuned parameters (including number of clusters) fitted on the entire dataset is 0.867\n"
     ]
    }
   ],
   "source": [
    "# Build the KMeans with 5 cluster and fit it to the dataset\n",
    "kmeans = KMeans(n_clusters = 5,random_state = 21)\n",
    "kmeans.fit(X)\n",
    "# Build the new predictor and concatenate it to the predictors DataFrame\n",
    "new_column = kmeans.predict(X)\n",
    "X_extended = pd.concat([X,pd.DataFrame(new_column,columns=['cluster'])],axis=1)\n",
    "# Build the random forest with the tuned parameters and fit it to the dataset\n",
    "model = RandomForestRegressor(n_estimators = 300, min_samples_leaf = 2,criterion = 'mse', max_features = 'sqrt', random_state=21)\n",
    "model.fit(X_extended,y)\n",
    "# Compute the R-squared for the random forest and print the score\n",
    "score = model.score(X_extended,y)\n",
    "print('The R-squared of the random forest with tuned parameters (including number of clusters) \\\n",
    "fitted on the entire dataset is %.3f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is 0.595\n",
      "The best parameters are:  {'C': 2, 'gamma': 0.01}\n",
      "The test set score after tuning parameters is 0.615\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=21)\n",
    "svr = SVR(kernel = 'rbf')\n",
    "parameters = {'C':[0.2,0.5,1,2],'gamma':[0.01,0.05,0.1]}\n",
    "model = GridSearchCV(svr,param_grid = parameters, cv=5)\n",
    "model.fit(X_train,y_train)\n",
    "print('The best score is %.3f'%model.best_score_)\n",
    "print('The best parameters are: ',model.best_params_)\n",
    "print('The test set score after tuning parameters is %.3f'%model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is 0.637\n",
      "The best parameters are:  {'C': 80, 'gamma': 0.002}\n",
      "The test set score after tuning parameters is 0.633\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(kernel = 'rbf')\n",
    "parameters = {'C':[30,50,70,80],'gamma':[0.002,0.003,0.004,0.005]}\n",
    "model = GridSearchCV(svr,param_grid = parameters, cv=5)\n",
    "model.fit(X_train,y_train)\n",
    "print('The best score is %.3f'%model.best_score_)\n",
    "print('The best parameters are: ',model.best_params_)\n",
    "print('The test set score after tuning parameters is %.3f'%model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is 0.584\n",
      "The best parameters are:  {'C': 30}\n",
      "The test set score after tuning parameters is 0.573\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(kernel = 'linear')\n",
    "parameters = {'C':[30,50,70,80]}\n",
    "model = GridSearchCV(svr,param_grid = parameters, cv=5)\n",
    "model.fit(X_train,y_train)\n",
    "print('The best score is %.3f'%model.best_score_)\n",
    "print('The best parameters are: ',model.best_params_)\n",
    "print('The test set score after tuning parameters is %.3f'%model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
